# agente_eda.py CORRIGIDO - PARTE 1: IMPORTS E CONFIGURA√á√ÉO
import os
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import warnings
warnings.filterwarnings('ignore')

# Imports do LangChain
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from langchain.tools import tool

# Carregar configura√ß√µes
load_dotenv()

print("üöÄ Iniciando Agente EDA...")

# Vari√°veis globais simples
dataset_atual = None
descobertas_memoria = []

# Configurar LLM
llm = ChatOpenAI(
    model=os.getenv('OPENAI_MODEL', 'gpt-4o-mini'),
    temperature=0.1,
    api_key=os.getenv('OPENAI_API_KEY')
)

print(f"ü§ñ LLM configurado: {os.getenv('OPENAI_MODEL', 'gpt-4o-mini')}")
print("‚úÖ Agente b√°sico inicializado!")

# ===== FERRAMENTAS INTELIGENTES =====
# agente_eda.py CORRIGIDO - PARTE 2: FERRAMENTA CARREGAR CSV

@tool
def carregar_csv(caminho_arquivo: str) -> str:
    """
    üîß Carrega um arquivo CSV e faz an√°lise inicial autom√°tica.
    
    Args:
        caminho_arquivo (str): Caminho para o arquivo CSV (ex: 'data/creditcard.csv')
    
    Returns:
        str: Relat√≥rio da an√°lise inicial
    """
    global dataset_atual, descobertas_memoria
    
    try:
        print(f"üìä Carregando: {caminho_arquivo}")
        
        # Limpar gr√°ficos antigos ANTES de carregar novo dataset
        import glob
        graficos_antigos = glob.glob("grafico_*.png")
        for grafico in graficos_antigos:
            try:
                os.remove(grafico)
            except:
                pass
        
        # Carregar o dataset
        df = pd.read_csv(caminho_arquivo)
        dataset_atual = df
        
        # An√°lise inicial autom√°tica
        relatorio = f"""
üéØ DATASET CARREGADO COM SUCESSO!

üìã INFORMA√á√ïES B√ÅSICAS:
- Arquivo: {caminho_arquivo}
- Linhas: {df.shape[0]:,}
- Colunas: {df.shape[1]}
- Tamanho em mem√≥ria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB

üìä ESTRUTURA DOS DADOS:
- Colunas num√©ricas: {len(df.select_dtypes(include=[np.number]).columns)}
- Colunas de texto: {len(df.select_dtypes(include=['object']).columns)}
- Valores ausentes total: {df.isnull().sum().sum()}

üîç NOMES DAS COLUNAS:
{', '.join(df.columns.tolist())}

üß† DETEC√á√ÉO AUTOM√ÅTICA DE TIPO:"""

        # Auto-detec√ß√£o inteligente MELHORADA do tipo de dataset
        colunas_lower = [col.lower() for col in df.columns]
        colunas_texto = ' '.join(colunas_lower)
        
        # Detec√ß√£o de FRAUDE
        if 'class' in colunas_lower and any('v' in col.lower() for col in df.columns):
            tipo_detectado = "DETEC√á√ÉO DE FRAUDE DE CART√ÉO DE CR√âDITO"
            relatorio += f"""
- Tipo detectado: {tipo_detectado}
- An√°lises recomendadas: distribui√ß√£o de fraudes, padr√µes nas vari√°veis V, an√°lise de valores
- Foco especial: desbalanceamento de classes, outliers, correla√ß√µes
"""
        
        # Detec√ß√£o de VENDAS/COMERCIAL
        elif any(palavra in colunas_texto for palavra in ['sales', 'price', 'revenue', 'product', 'quantity', 'customer']):
            tipo_detectado = "DADOS DE VENDAS/COMERCIAL"
            relatorio += f"""
- Tipo detectado: {tipo_detectado}
- An√°lises recomendadas: tend√™ncias de vendas, an√°lise por produto, performance comercial
- Foco especial: sazonalidade, ranking de produtos, an√°lise de receita
"""
        
        # Detec√ß√£o de RH/RECURSOS HUMANOS
        elif any(palavra in colunas_texto for palavra in ['salary', 'employee', 'department', 'age', 'years']):
            tipo_detectado = "DADOS DE RH/RECURSOS HUMANOS"
            relatorio += f"""
- Tipo detectado: {tipo_detectado}
- An√°lises recomendadas: an√°lise salarial, distribui√ß√£o por departamento, demographics
- Foco especial: equidade salarial, performance por √°rea, an√°lise de idade
"""
        
        # Detec√ß√£o de DADOS CIENT√çFICOS
        elif any(palavra in colunas_texto for palavra in ['species', 'petal', 'sepal', 'length', 'width', 'class']):
            tipo_detectado = "DADOS CIENT√çFICOS/EXPERIMENTAIS"
            relatorio += f"""
- Tipo detectado: {tipo_detectado}
- An√°lises recomendadas: distribui√ß√µes por classe, correla√ß√µes entre medidas
- Foco especial: classifica√ß√£o de esp√©cies, an√°lise morfom√©trica, clusters
"""
        
        # Detec√ß√£o de DADOS TEMPORAIS
        elif any(palavra in colunas_texto for palavra in ['date', 'time', 'timestamp', 'year', 'month']):
            tipo_detectado = "DADOS TEMPORAIS/S√âRIES TEMPORAIS"
            relatorio += f"""
- Tipo detectado: {tipo_detectado}
- An√°lises recomendadas: tend√™ncias temporais, sazonalidade, previs√µes
- Foco especial: an√°lise de s√©ries, detec√ß√£o de padr√µes, decomposi√ß√£o temporal
"""
        
        # Fallback para DADOS GERAIS
        else:
            tipo_detectado = "DATASET GERAL"
            relatorio += f"""
- Tipo detectado: {tipo_detectado}
- An√°lises recomendadas: estat√≠sticas descritivas, correla√ß√µes, distribui√ß√µes
- Foco especial: an√°lise explorat√≥ria abrangente, identifica√ß√£o de padr√µes
"""
        
        # Salvar descoberta na mem√≥ria
        descoberta = f"Dataset carregado: {caminho_arquivo} ({df.shape[0]} linhas, tipo: {tipo_detectado})"
        descobertas_memoria.append(descoberta)
        
        return relatorio
        
    except Exception as e:
        return f"‚ùå ERRO ao carregar {caminho_arquivo}: {str(e)}"

print("üîß Ferramenta 'carregar_csv' criada!")
# agente_eda.py CORRIGIDO - PARTE 3: FERRAMENTA AN√ÅLISE AUTOM√ÅTICA

@tool
def analisar_automaticamente() -> str:
    """
    üß† Faz an√°lise autom√°tica completa do dataset carregado.
    O agente decide sozinho quais an√°lises fazer.
    
    Returns:
        str: Relat√≥rio completo da an√°lise autom√°tica
    """
    global dataset_atual, descobertas_memoria
    
    if dataset_atual is None:
        return "‚ùå Nenhum dataset carregado! Use 'carregar_csv' primeiro."
    
    df = dataset_atual
    relatorio = "üß† AN√ÅLISE AUTOM√ÅTICA INTELIGENTE\n" + "="*50 + "\n"
    
    try:
        # 1. Estat√≠sticas b√°sicas
        relatorio += "\nüìä 1. ESTAT√çSTICAS DESCRITIVAS:\n"
        colunas_numericas = df.select_dtypes(include=[np.number]).columns
        
        if len(colunas_numericas) > 0:
            desc = df[colunas_numericas].describe()
            relatorio += f"Colunas num√©ricas analisadas: {len(colunas_numericas)}\n"
            relatorio += desc.to_string()
        
        # 2. An√°lise espec√≠fica por tipo de dados
        colunas_lower = [col.lower() for col in df.columns]
        colunas_texto = ' '.join(colunas_lower)
        
        # An√°lise para FRAUDE
        if 'class' in colunas_lower and any('v' in col.lower() for col in df.columns):
            relatorio += "\n\nüéØ 2. AN√ÅLISE DE FRAUDE DETECTADA:\n"
            if 'Class' in df.columns:
                contagem = df['Class'].value_counts()
                total = len(df)
                
                relatorio += f"- Transa√ß√µes normais (0): {contagem[0]:,} ({contagem[0]/total*100:.2f}%)\n"
                relatorio += f"- Transa√ß√µes fraudulentas (1): {contagem[1]:,} ({contagem[1]/total*100:.2f}%)\n"
                relatorio += f"- Taxa de fraude: {contagem[1]/total*100:.4f}%\n"
                
                if contagem[1]/total < 0.01:
                    relatorio += "‚ö†Ô∏è  DATASET ALTAMENTE DESBALANCEADO - Fraudes s√£o muito raras!\n"
        
        # An√°lise para VENDAS
        elif any(palavra in colunas_texto for palavra in ['sales', 'price', 'revenue', 'product', 'quantity']):
            relatorio += "\n\nüè™ 2. AN√ÅLISE DE VENDAS DETECTADA:\n"
            
            # Procurar colunas de vendas
            colunas_vendas = [col for col in df.columns if any(palavra in col.lower() for palavra in ['sales', 'revenue', 'price', 'amount'])]
            if colunas_vendas:
                col_vendas = colunas_vendas[0]
                vendas = df[col_vendas]
                relatorio += f"- Coluna de vendas identificada: {col_vendas}\n"
                relatorio += f"- Vendas totais: ${vendas.sum():,.2f}\n"
                relatorio += f"- Vendas m√©dias: ${vendas.mean():.2f}\n"
                relatorio += f"- Maior venda: ${vendas.max():.2f}\n"
                relatorio += f"- Menor venda: ${vendas.min():.2f}\n"
            
            # Procurar produtos
            colunas_produto = [col for col in df.columns if any(palavra in col.lower() for palavra in ['product', 'item', 'categoria'])]
            if colunas_produto:
                col_produto = colunas_produto[0]
                produtos_unicos = df[col_produto].nunique()
                relatorio += f"- Produtos √∫nicos: {produtos_unicos}\n"
                top_produtos = df[col_produto].value_counts().head(3)
                relatorio += f"- Top 3 produtos mais frequentes: {', '.join(top_produtos.index.tolist())}\n"
        
        # An√°lise para DADOS CIENT√çFICOS
        elif any(palavra in colunas_texto for palavra in ['species', 'petal', 'sepal', 'length', 'width']):
            relatorio += "\n\nüî¨ 2. AN√ÅLISE CIENT√çFICA DETECTADA:\n"
            
            # Procurar coluna de classes/esp√©cies
            colunas_classe = [col for col in df.columns if any(palavra in col.lower() for palavra in ['species', 'class', 'tipo'])]
            if colunas_classe:
                col_classe = colunas_classe[0]
                classes = df[col_classe].value_counts()
                relatorio += f"- Classes/Esp√©cies identificadas: {classes.index.tolist()}\n"
                relatorio += f"- Distribui√ß√£o por classe:\n"
                for classe, count in classes.items():
                    relatorio += f"  * {classe}: {count} ({count/len(df)*100:.1f}%)\n"
            
            # An√°lise de medidas morfom√©tricas
            colunas_medidas = [col for col in df.columns if any(palavra in col.lower() for palavra in ['length', 'width', 'height', 'petal', 'sepal'])]
            if len(colunas_medidas) >= 2:
                relatorio += f"- Medidas morfom√©tricas encontradas: {', '.join(colunas_medidas)}\n"
                correlacao_max = df[colunas_medidas].corr().abs().max().max()
                relatorio += f"- Correla√ß√£o m√°xima entre medidas: {correlacao_max:.3f}\n"
        
        # 3. An√°lise de valores ausentes
        relatorio += "\n\nüîç 3. VALORES AUSENTES:\n"
        valores_ausentes = df.isnull().sum()
        if valores_ausentes.sum() == 0:
            relatorio += "‚úÖ Nenhum valor ausente encontrado!\n"
        else:
            relatorio += "‚ö†Ô∏è  Valores ausentes encontrados:\n"
            for col, missing in valores_ausentes[valores_ausentes > 0].items():
                relatorio += f"   - {col}: {missing} ({missing/len(df)*100:.2f}%)\n"
        
        # 4. An√°lise de correla√ß√µes (para dados num√©ricos)
        if len(colunas_numericas) > 1:
            relatorio += "\n\nüîó 4. AN√ÅLISE DE CORRELA√á√ïES:\n"
            corr_matrix = df[colunas_numericas].corr()
            
            # Encontrar correla√ß√µes mais fortes (excluindo diagonal)
            corr_values = corr_matrix.abs().values
            np.fill_diagonal(corr_values, 0)
            max_corr = np.max(corr_values)
            max_idx = np.unravel_index(np.argmax(corr_values), corr_values.shape)
            
            col1 = colunas_numericas[max_idx[0]]
            col2 = colunas_numericas[max_idx[1]]
            
            relatorio += f"- Correla√ß√£o mais forte: {col1} vs {col2} ({max_corr:.3f})\n"
            
            if max_corr > 0.7:
                relatorio += "‚ö†Ô∏è  Correla√ß√£o muito alta detectada - poss√≠vel multicolinearidade\n"
            elif max_corr > 0.5:
                relatorio += "üí° Correla√ß√£o moderada detectada - vari√°veis relacionadas\n"
            else:
                relatorio += "‚úÖ Correla√ß√µes baixas - vari√°veis independentes\n"
        
        # 5. Insights autom√°ticos MELHORADOS
        relatorio += "\n\nüß† 5. INSIGHTS AUTOM√ÅTICOS:\n"
        insights = []
        
        # Insights para FRAUDE
        if 'Class' in df.columns:
            fraud_rate = df['Class'].sum() / len(df)
            if fraud_rate < 0.001:
                insights.append("- Dataset extremamente desbalanceado - t√©cnicas especiais podem ser necess√°rias")
            
            if 'Amount' in df.columns:
                normal_avg = df[df['Class'] == 0]['Amount'].mean()
                fraud_avg = df[df['Class'] == 1]['Amount'].mean()
                if fraud_avg < normal_avg:
                    insights.append("- Transa√ß√µes fraudulentas tendem a ter valores MENORES que as normais")
                else:
                    insights.append("- Transa√ß√µes fraudulentas tendem a ter valores MAIORES que as normais")
        
        # Insights para VENDAS
        elif any(palavra in colunas_texto for palavra in ['sales', 'price', 'revenue']):
            colunas_vendas = [col for col in df.columns if any(palavra in col.lower() for palavra in ['sales', 'revenue', 'price'])]
            if colunas_vendas:
                col_vendas = colunas_vendas[0]
                cv = df[col_vendas].std() / df[col_vendas].mean()
                if cv > 1:
                    insights.append("- Alta variabilidade nas vendas - mercado inst√°vel ou sazonalidade")
                else:
                    insights.append("- Vendas com variabilidade moderada - padr√£o consistente")
        
        # Insights para DADOS CIENT√çFICOS
        elif any(palavra in colunas_texto for palavra in ['species', 'petal', 'sepal']):
            insights.append("- Dataset cient√≠fico identificado - ideal para an√°lise de classifica√ß√£o")
            if len(colunas_numericas) >= 4:
                insights.append("- M√∫ltiplas medidas dispon√≠veis - poss√≠vel an√°lise multivariada")
        
        # Insights GERAIS
        if len(colunas_numericas) > len(df.select_dtypes(include=['object']).columns):
            insights.append("- Dataset predominantemente num√©rico - ideal para an√°lises estat√≠sticas")
        
        # Verificar vari√°veis PCA (V1, V2, etc.)
        v_columns = [col for col in df.columns if col.startswith('V')]
        if len(v_columns) > 10:
            insights.append(f"- Dataset cont√©m {len(v_columns)} vari√°veis transformadas por PCA (V1-V{len(v_columns)})")
            insights.append("- Essas vari√°veis s√£o resultado de transforma√ß√£o para proteger dados sens√≠veis")
        
        # Insights sobre tamanho do dataset
        if len(df) > 100000:
            insights.append("- Dataset grande (>100k linhas) - an√°lises robustas poss√≠veis")
        elif len(df) < 1000:
            insights.append("- Dataset pequeno (<1k linhas) - cuidado com generaliza√ß√µes")
        
        for insight in insights:
            relatorio += f"{insight}\n"
        
        # Salvar na mem√≥ria
        descoberta = f"An√°lise autom√°tica realizada: {len(insights)} insights gerados"
        descobertas_memoria.append(descoberta)
        
        return relatorio
        
    except Exception as e:
        return f"‚ùå ERRO na an√°lise autom√°tica: {str(e)}"

print("üß† Ferramenta 'analisar_automaticamente' criada!")
# agente_eda.py CORRIGIDO - PARTE 4: FERRAMENTA GR√ÅFICOS (ARQUIVO √öNICO)

@tool
def criar_grafico_automatico(tipo_analise: str = "auto") -> str:
    """
    üìä Cria gr√°ficos autom√°ticos baseados no tipo de an√°lise solicitada.
    
    Args:
        tipo_analise (str): Tipo de gr√°fico - "auto", "distribuicao", "correlacao", "valores"
    
    Returns:
        str: Relat√≥rio sobre o gr√°fico criado
    """
    global dataset_atual, descobertas_memoria
    
    if dataset_atual is None:
        return "‚ùå Nenhum dataset carregado! Use 'carregar_csv' primeiro."
    
    df = dataset_atual
    
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        plt.style.use('default')
        colunas_lower = [col.lower() for col in df.columns]
        colunas_texto = ' '.join(colunas_lower)
        
        # CORRE√á√ÉO: Auto-detec√ß√£o do tipo de gr√°fico apropriado
        if tipo_analise == "auto":
            # Para dados de FRAUDE
            if 'class' in colunas_lower and any('v' in col.lower() for col in df.columns) and 'Class' in df.columns:
                tipo_analise = "distribuicao_fraude"
            
            # Para dados de VENDAS
            elif any(palavra in colunas_texto for palavra in ['sales', 'price', 'revenue', 'product', 'quantity']):
                tipo_analise = "vendas_analise"
            
            # Para dados CIENT√çFICOS
            elif any(palavra in colunas_texto for palavra in ['species', 'petal', 'sepal']):
                tipo_analise = "cientifico_analise"
            
            # Para dados GERAIS - usar correla√ß√£o
            else:
                tipo_analise = "correlacao"
        
        # GR√ÅFICO PARA FRAUDE
        if tipo_analise == "distribuicao_fraude" and 'Class' in df.columns:
            plt.figure(figsize=(12, 5))
            
            # Subplot 1: Pizza
            plt.subplot(1, 3, 1)
            contagem = df['Class'].value_counts()
            colors = ['lightblue', 'red']
            plt.pie(contagem.values, labels=['Normal (0)', 'Fraude (1)'], 
                   autopct='%1.2f%%', colors=colors, startangle=90)
            plt.title('Distribui√ß√£o de Classes')
            
            # Subplot 2: Barras
            plt.subplot(1, 3, 2)
            plt.bar(['Normal', 'Fraude'], contagem.values, color=colors)
            plt.title('Contagem por Tipo')
            plt.ylabel('Quantidade')
            
            # Subplot 3: Valores (se Amount existe)
            if 'Amount' in df.columns:
                plt.subplot(1, 3, 3)
                normal_amount = df[df['Class'] == 0]['Amount']
                fraud_amount = df[df['Class'] == 1]['Amount']
                
                plt.boxplot([normal_amount, fraud_amount], labels=['Normal', 'Fraude'])
                plt.title('Distribui√ß√£o de Valores')
                plt.ylabel('Valor ($)')
                plt.yscale('log')
            
            plt.tight_layout()
            # CORRE√á√ÉO: Nome √∫nico do arquivo
            plt.savefig('grafico_atual.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            descoberta = "Gr√°fico de fraude criado: grafico_atual.png"
            descobertas_memoria.append(descoberta)
            
            return "üìä GR√ÅFICO DE FRAUDE CRIADO! Arquivo: grafico_atual.png - An√°lise completa de distribui√ß√£o e valores."
        
        # GR√ÅFICO PARA VENDAS (CORRIGIDO)
        elif tipo_analise == "vendas_analise":
            plt.figure(figsize=(15, 5))
            
            # Encontrar colunas relevantes para vendas
            colunas_vendas = [col for col in df.columns if any(palavra in col.lower() for palavra in ['sales', 'revenue', 'price', 'amount'])]
            colunas_produto = [col for col in df.columns if any(palavra in col.lower() for palavra in ['product', 'item', 'categoria', 'category'])]
            
            if colunas_vendas:
                col_vendas = colunas_vendas[0]
                
                # Subplot 1: Histograma de vendas
                plt.subplot(1, 3, 1)
                plt.hist(df[col_vendas], bins=30, color='green', alpha=0.7)
                plt.title(f'Distribui√ß√£o de {col_vendas}')
                plt.xlabel(col_vendas)
                plt.ylabel('Frequ√™ncia')
                
                # Subplot 2: Top produtos (se dispon√≠vel)
                if colunas_produto:
                    plt.subplot(1, 3, 2)
                    col_produto = colunas_produto[0]
                    top_produtos = df[col_produto].value_counts().head(10)
                    y_pos = range(len(top_produtos))
                    plt.barh(y_pos, top_produtos.values, color='orange')
                    plt.yticks(y_pos, [str(x)[:20] + '...' if len(str(x)) > 20 else str(x) for x in top_produtos.index])
                    plt.title(f'Top 10 {col_produto}')
                    plt.xlabel('Quantidade')
                
                # Subplot 3: Box plot de vendas
                plt.subplot(1, 3, 3)
                plt.boxplot(df[col_vendas])
                plt.title(f'Box Plot - {col_vendas}')
                plt.ylabel(col_vendas)
            
            plt.tight_layout()
            # CORRE√á√ÉO: Nome √∫nico do arquivo
            plt.savefig('grafico_atual.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            descoberta = "Gr√°fico de vendas criado: grafico_atual.png"
            descobertas_memoria.append(descoberta)
            
            return "üìä GR√ÅFICO DE VENDAS CRIADO! Arquivo: grafico_atual.png - An√°lise completa de vendas e produtos."
        
        # GR√ÅFICO PARA DADOS CIENT√çFICOS
        elif tipo_analise == "cientifico_analise":
            plt.figure(figsize=(15, 5))
            
            colunas_medidas = [col for col in df.columns if any(palavra in col.lower() for palavra in ['length', 'width', 'petal', 'sepal'])]
            colunas_classe = [col for col in df.columns if any(palavra in col.lower() for palavra in ['species', 'class'])]
            
            if len(colunas_medidas) >= 2:
                # Subplot 1: Scatter plot
                plt.subplot(1, 3, 1)
                if colunas_classe:
                    classes = df[colunas_classe[0]].unique()
                    colors = ['red', 'blue', 'green', 'orange', 'purple']
                    for i, classe in enumerate(classes):
                        mask = df[colunas_classe[0]] == classe
                        plt.scatter(df[mask][colunas_medidas[0]], df[mask][colunas_medidas[1]], 
                                  c=colors[i % len(colors)], label=classe, alpha=0.7)
                    plt.legend()
                else:
                    plt.scatter(df[colunas_medidas[0]], df[colunas_medidas[1]], alpha=0.7)
                
                plt.xlabel(colunas_medidas[0])
                plt.ylabel(colunas_medidas[1])
                plt.title('Scatter Plot - Medidas')
                
                # Subplot 2: Histograma das medidas
                plt.subplot(1, 3, 2)
                for i, col in enumerate(colunas_medidas[:4]):
                    plt.hist(df[col], alpha=0.5, label=col, bins=20)
                plt.legend()
                plt.title('Distribui√ß√µes das Medidas')
                plt.xlabel('Valores')
                plt.ylabel('Frequ√™ncia')
                
                # Subplot 3: Box plot por classe
                if colunas_classe and len(colunas_medidas) >= 1:
                    plt.subplot(1, 3, 3)
                    classes = df[colunas_classe[0]].unique()
                    data_boxplot = []
                    labels_boxplot = []
                    
                    for classe in classes:
                        mask = df[colunas_classe[0]] == classe
                        data_boxplot.append(df[mask][colunas_medidas[0]])
                        labels_boxplot.append(classe)
                    
                    plt.boxplot(data_boxplot, labels=labels_boxplot)
                    plt.title(f'{colunas_medidas[0]} por {colunas_classe[0]}')
                    plt.ylabel(colunas_medidas[0])
            
            plt.tight_layout()
            # CORRE√á√ÉO: Nome √∫nico do arquivo
            plt.savefig('grafico_atual.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            descoberta = "Gr√°fico cient√≠fico criado: grafico_atual.png"
            descobertas_memoria.append(descoberta)
            
            return "üìä GR√ÅFICO CIENT√çFICO CRIADO! Arquivo: grafico_atual.png - An√°lise de medidas e classifica√ß√µes."
        
        # GR√ÅFICO DE CORRELA√á√ÉO (GERAL) - CORRIGIDO
        elif tipo_analise == "correlacao":
            colunas_numericas = df.select_dtypes(include=[np.number]).columns
            
            if len(colunas_numericas) > 1:
                plt.figure(figsize=(12, 8))
                
                # Calcular matriz de correla√ß√£o
                corr_matrix = df[colunas_numericas].corr()
                
                # Criar heatmap
                mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
                sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                           square=True, linewidths=0.5, cbar_kws={"shrink": 0.5}, fmt='.2f')
                
                plt.title('Matriz de Correla√ß√£o')
                plt.tight_layout()
                # CORRE√á√ÉO: Nome √∫nico do arquivo
                plt.savefig('grafico_atual.png', dpi=300, bbox_inches='tight')
                plt.close()
                
                descoberta = "Gr√°fico de correla√ß√£o criado: grafico_atual.png"
                descobertas_memoria.append(descoberta)
                
                return "üìä GR√ÅFICO DE CORRELA√á√ÉO CRIADO! Arquivo: grafico_atual.png - Matriz de correla√ß√£o entre vari√°veis num√©ricas."
            
            else:
                return "‚ùå Dados insuficientes para criar matriz de correla√ß√£o (precisa de 2+ colunas num√©ricas)."
        
        # FALLBACK - Gr√°fico simples de distribui√ß√µes (CORRIGIDO)
        else:
            colunas_numericas = df.select_dtypes(include=[np.number]).columns
            
            if len(colunas_numericas) > 0:
                plt.figure(figsize=(12, 8))
                
                # Histogramas das primeiras 6 colunas num√©ricas
                n_cols = min(6, len(colunas_numericas))
                rows = 2
                cols = 3
                
                for i, col in enumerate(colunas_numericas[:n_cols]):
                    plt.subplot(rows, cols, i+1)
                    plt.hist(df[col], bins=20, alpha=0.7, color=f'C{i}')
                    plt.title(f'Distribui√ß√£o - {col}')
                    plt.xlabel(col)
                    plt.ylabel('Frequ√™ncia')
                
                plt.tight_layout()
                # CORRE√á√ÉO: Nome √∫nico do arquivo
                plt.savefig('grafico_atual.png', dpi=300, bbox_inches='tight')
                plt.close()
                
                descoberta = "Gr√°fico geral criado: grafico_atual.png"
                descobertas_memoria.append(descoberta)
                
                return "üìä GR√ÅFICO GERAL CRIADO! Arquivo: grafico_atual.png - Distribui√ß√µes das principais vari√°veis."
            
            else:
                return "‚ùå Nenhuma coluna num√©rica encontrada para criar gr√°ficos."
            
    except Exception as e:
        return f"‚ùå ERRO ao criar gr√°fico: {str(e)}"

print("üìä Ferramenta 'criar_grafico_automatico' criada!")
# agente_eda.py CORRIGIDO - PARTE 5: FERRAMENTAS AUXILIARES

@tool
def analisar_variavel_especifica(nome_variavel: str, tipo_analise: str = "completa") -> str:
    """
    üîç Analisa uma vari√°vel espec√≠fica do dataset carregado.
    
    Args:
        nome_variavel (str): Nome exato da coluna a ser analisada
        tipo_analise (str): "completa", "distribuicao", "outliers", "estatisticas"
    
    Returns:
        str: An√°lise detalhada da vari√°vel
    """
    global dataset_atual
    
    if dataset_atual is None:
        return "‚ùå Nenhum dataset carregado! Use 'carregar_csv' primeiro."
    
    df = dataset_atual
    
    # Verificar se a vari√°vel existe
    if nome_variavel not in df.columns:
        colunas_similares = [col for col in df.columns if nome_variavel.lower() in col.lower()]
        if colunas_similares:
            return f"‚ùå Vari√°vel '{nome_variavel}' n√£o encontrada. Voc√™ quis dizer: {', '.join(colunas_similares)}?"
        else:
            return f"‚ùå Vari√°vel '{nome_variavel}' n√£o existe. Colunas dispon√≠veis: {', '.join(df.columns.tolist())}"
    
    var = df[nome_variavel]
    
    relatorio = f"üîç AN√ÅLISE DA VARI√ÅVEL: {nome_variavel}\n" + "="*40 + "\n"
    
    # Informa√ß√µes b√°sicas
    relatorio += f"\nüìä INFORMA√á√ïES B√ÅSICAS:\n"
    relatorio += f"- Tipo de dados: {var.dtype}\n"
    relatorio += f"- Valores √∫nicos: {var.nunique():,}\n"
    relatorio += f"- Valores n√£o-nulos: {var.count():,}\n"
    relatorio += f"- Valores ausentes: {var.isnull().sum()}\n"
    
    if tipo_analise in ["completa", "estatisticas"]:
        # Estat√≠sticas para vari√°veis num√©ricas
        if var.dtype in ['int64', 'float64']:
            relatorio += f"\nüìà ESTAT√çSTICAS DESCRITIVAS:\n"
            relatorio += f"- M√©dia: {var.mean():.4f}\n"
            relatorio += f"- Mediana: {var.median():.4f}\n"
            relatorio += f"- M√≠nimo: {var.min():.4f}\n"
            relatorio += f"- M√°ximo: {var.max():.4f}\n"
            relatorio += f"- Desvio padr√£o: {var.std():.4f}\n"
            relatorio += f"- Vari√¢ncia: {var.var():.4f}\n"
            relatorio += f"- Assimetria: {var.skew():.4f}\n"
            relatorio += f"- Curtose: {var.kurtosis():.4f}\n"
            
            # Quartis
            q1 = var.quantile(0.25)
            q3 = var.quantile(0.75)
            iqr = q3 - q1
            relatorio += f"- Q1 (25%): {q1:.4f}\n"
            relatorio += f"- Q3 (75%): {q3:.4f}\n"
            relatorio += f"- IQR: {iqr:.4f}\n"
        
        # Estat√≠sticas para vari√°veis categ√≥ricas
        else:
            relatorio += f"\nüìù AN√ÅLISE CATEG√ìRICA:\n"
            value_counts = var.value_counts().head(10)
            relatorio += f"- Top 10 valores mais frequentes:\n"
            for valor, freq in value_counts.items():
                relatorio += f"  * {valor}: {freq} ({freq/len(var)*100:.2f}%)\n"
    
    if tipo_analise in ["completa", "outliers"]:
        # Detec√ß√£o de outliers (apenas para vari√°veis num√©ricas)
        if var.dtype in ['int64', 'float64']:
            relatorio += f"\nüö® DETEC√á√ÉO DE OUTLIERS:\n"
            
            # M√©todo IQR
            q1 = var.quantile(0.25)
            q3 = var.quantile(0.75)
            iqr = q3 - q1
            limite_inferior = q1 - 1.5 * iqr
            limite_superior = q3 + 1.5 * iqr
            
            outliers = var[(var < limite_inferior) | (var > limite_superior)]
            relatorio += f"- Outliers detectados (IQR): {len(outliers)}\n"
            
            if len(outliers) > 0:
                relatorio += f"- Percentual de outliers: {len(outliers)/len(var)*100:.2f}%\n"
                relatorio += f"- Limite inferior: {limite_inferior:.4f}\n"
                relatorio += f"- Limite superior: {limite_superior:.4f}\n"
                
                if len(outliers) <= 10:
                    relatorio += f"- Valores outliers: {outliers.tolist()}\n"
                else:
                    relatorio += f"- Primeiros 5 outliers: {outliers.head().tolist()}\n"
    
    return relatorio

print("üîç Ferramenta 'analisar_variavel_especifica' criada!")

@tool
def obter_contexto_atual() -> str:
    """
    üß† Obt√©m informa√ß√µes sobre o dataset atualmente carregado.
    
    Returns:
        str: Contexto atual do dataset
    """
    global dataset_atual, descobertas_memoria
    
    if dataset_atual is None:
        return "‚ùå Nenhum dataset carregado no momento."
    
    df = dataset_atual
    
    # Detectar tipo
    colunas_lower = [col.lower() for col in df.columns]
    colunas_texto = ' '.join(colunas_lower)
    
    if any(palavra in colunas_texto for palavra in ['sales', 'price', 'revenue', 'product']):
        contexto = "Este √© um dataset de VENDAS COMERCIAIS com informa√ß√µes sobre produtos, clientes e transa√ß√µes de vendas."
    elif 'class' in colunas_lower and any('v' in col.lower() for col in df.columns):
        contexto = "Este √© um dataset de DETEC√á√ÉO DE FRAUDE de cart√£o de cr√©dito."
    elif any(palavra in colunas_texto for palavra in ['species', 'petal', 'sepal']):
        contexto = "Este √© um dataset CIENT√çFICO com dados de classifica√ß√£o e medidas."
    else:
        contexto = "Este √© um dataset GERAL para an√°lise explorat√≥ria."
    
    relatorio = f"""
üîç SOBRE ESTA TABELA:

üìä CONTEXTO:
{contexto}

üìã CARACTER√çSTICAS:
- Linhas: {df.shape[0]:,}
- Colunas: {df.shape[1]}
- Colunas: {', '.join(df.columns.tolist())}
"""
    
    return relatorio

print("üß† Ferramenta 'obter_contexto_atual' criada!")
# agente_eda.py CORRIGIDO - PARTE 6: FERRAMENTAS AVAN√áADAS

@tool
def analisar_tendencias_temporais(coluna_data: str = "auto", coluna_valor: str = "auto") -> str:
    """
    üìÖ Analisa tend√™ncias temporais nos dados.
    
    Args:
        coluna_data (str): Nome da coluna de data/tempo ("auto" para detec√ß√£o autom√°tica)
        coluna_valor (str): Nome da coluna de valores ("auto" para detec√ß√£o autom√°tica)
    
    Returns:
        str: An√°lise de tend√™ncias temporais
    """
    global dataset_atual, descobertas_memoria
    
    if dataset_atual is None:
        return "‚ùå Nenhum dataset carregado! Use 'carregar_csv' primeiro."
    
    df = dataset_atual
    
    try:
        # Auto-detec√ß√£o de colunas temporais
        if coluna_data == "auto":
            colunas_temporais = []
            for col in df.columns:
                col_lower = col.lower()
                if any(palavra in col_lower for palavra in ['date', 'time', 'timestamp', 'year', 'month', 'day']):
                    colunas_temporais.append(col)
            
            if not colunas_temporais:
                return "‚ùå Nenhuma coluna temporal detectada. Colunas dispon√≠veis: " + ", ".join(df.columns.tolist())
            
            coluna_data = colunas_temporais[0]
        
        # Auto-detec√ß√£o de coluna de valores
        if coluna_valor == "auto":
            colunas_valor = []
            for col in df.columns:
                if df[col].dtype in ['int64', 'float64'] and col != coluna_data:
                    col_lower = col.lower()
                    if any(palavra in col_lower for palavra in ['sales', 'amount', 'price', 'revenue', 'value']):
                        colunas_valor.append(col)
            
            if not colunas_valor:
                # Pegar primeira coluna num√©rica que n√£o √© a data
                colunas_numericas = df.select_dtypes(include=[np.number]).columns
                colunas_valor = [col for col in colunas_numericas if col != coluna_data]
            
            if not colunas_valor:
                return "‚ùå Nenhuma coluna de valores num√©ricos encontrada para an√°lise temporal."
            
            coluna_valor = colunas_valor[0]
        
        # Verificar se as colunas existem
        if coluna_data not in df.columns:
            return f"‚ùå Coluna de data '{coluna_data}' n√£o encontrada."
        if coluna_valor not in df.columns:
            return f"‚ùå Coluna de valores '{coluna_valor}' n√£o encontrada."
        
        relatorio = f"üìÖ AN√ÅLISE TEMPORAL\n" + "="*40 + "\n"
        relatorio += f"\nüîç COLUNAS ANALISADAS:\n"
        relatorio += f"- Coluna temporal: {coluna_data}\n"
        relatorio += f"- Coluna de valores: {coluna_valor}\n"
        
        # Tentar converter para datetime
        try:
            df_temp = df.copy()
            df_temp[coluna_data] = pd.to_datetime(df_temp[coluna_data])
            
            # An√°lise temporal b√°sica
            relatorio += f"\nüìä AN√ÅLISE TEMPORAL:\n"
            relatorio += f"- Per√≠odo inicial: {df_temp[coluna_data].min()}\n"
            relatorio += f"- Per√≠odo final: {df_temp[coluna_data].max()}\n"
            relatorio += f"- Dura√ß√£o total: {(df_temp[coluna_data].max() - df_temp[coluna_data].min()).days} dias\n"
            
            # Criar gr√°fico temporal
            import matplotlib.pyplot as plt
            plt.figure(figsize=(12, 6))
            
            # Agrupar por m√™s para visualiza√ß√£o
            df_mensal = df_temp.groupby([df_temp[coluna_data].dt.to_period('M')])[coluna_valor].sum()
            
            plt.subplot(1, 2, 1)
            df_mensal.plot(kind='line', color='blue')
            plt.title(f'Tend√™ncia Temporal - {coluna_valor}')
            plt.xlabel('Per√≠odo')
            plt.ylabel(coluna_valor)
            plt.xticks(rotation=45)
            
            # Histograma por m√™s do ano (sazonalidade)
            plt.subplot(1, 2, 2)
            df_temp['mes'] = df_temp[coluna_data].dt.month
            sazonalidade = df_temp.groupby('mes')[coluna_valor].mean()
            meses = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun', 
                    'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez']
            plt.bar(range(1, 13), [sazonalidade.get(i, 0) for i in range(1, 13)], color='green')
            plt.title('Sazonalidade por M√™s')
            plt.xlabel('M√™s')
            plt.ylabel(f'M√©dia {coluna_valor}')
            plt.xticks(range(1, 13), meses, rotation=45)
            
            plt.tight_layout()
            # CORRE√á√ÉO: Nome √∫nico do arquivo
            plt.savefig('grafico_atual.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            descoberta = f"An√°lise temporal realizada: {coluna_data} vs {coluna_valor}"
            descobertas_memoria.append(descoberta)
            
            relatorio += f"\nüìä GR√ÅFICO TEMPORAL CRIADO: grafico_atual.png\n"
            
            return relatorio
            
        except Exception as e_date:
            # Se n√£o conseguir converter para data, an√°lise b√°sica
            relatorio += f"\n‚ö†Ô∏è N√£o foi poss√≠vel converter '{coluna_data}' para formato de data.\n"
            relatorio += f"Fazendo an√°lise b√°sica da sequ√™ncia de valores...\n"
            
            # An√°lise de tend√™ncia simples (assumindo ordem temporal)
            valores = df[coluna_valor]
            if len(valores) > 1:
                primeira_metade = valores[:len(valores)//2].mean()
                segunda_metade = valores[len(valores)//2:].mean()
                mudanca = ((segunda_metade - primeira_metade) / primeira_metade) * 100
                
                relatorio += f"- Valor m√©dio primeira metade: {primeira_metade:.2f}\n"
                relatorio += f"- Valor m√©dio segunda metade: {segunda_metade:.2f}\n"
                relatorio += f"- Mudan√ßa percentual: {mudanca:.2f}%\n"
                
                if abs(mudanca) > 10:
                    relatorio += f"‚ö†Ô∏è Tend√™ncia significativa detectada!\n"
                else:
                    relatorio += f"‚úÖ Valores relativamente est√°veis.\n"
            
            return relatorio
            
    except Exception as e:
        return f"‚ùå ERRO na an√°lise temporal: {str(e)}"

print("üìÖ Ferramenta 'analisar_tendencias_temporais' criada!")

@tool
def detectar_clusters(n_clusters: str = "auto", colunas: str = "auto") -> str:
    """
    üéØ Detecta agrupamentos (clusters) nos dados usando K-means.
    
    Args:
        n_clusters (str): N√∫mero de clusters ("auto" para detec√ß√£o autom√°tica, ou n√∫mero espec√≠fico)
        colunas (str): Colunas para usar ("auto" para sele√ß√£o autom√°tica, ou nomes separados por v√≠rgula)
    
    Returns:
        str: An√°lise de clusters encontrados
    """
    global dataset_atual, descobertas_memoria
    
    if dataset_atual is None:
        return "‚ùå Nenhum dataset carregado! Use 'carregar_csv' primeiro."
    
    df = dataset_atual
    
    try:
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import StandardScaler
        
        # Auto-sele√ß√£o de colunas num√©ricas
        if colunas == "auto":
            colunas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
            # Remover colunas de ID se existirem
            colunas_para_cluster = [col for col in colunas_numericas 
                                  if not any(palavra in col.lower() for palavra in ['id', 'index', 'row'])]
        else:
            colunas_para_cluster = [col.strip() for col in colunas.split(',')]
        
        if len(colunas_para_cluster) < 2:
            return "‚ùå Precisa de pelo menos 2 colunas num√©ricas para an√°lise de clusters."
        
        # Preparar dados (remover NaN)
        dados_cluster = df[colunas_para_cluster].dropna()
        
        if len(dados_cluster) < 10:
            return "‚ùå Dados insuficientes para an√°lise de clusters (m√≠nimo 10 linhas sem NaN)."
        
        # Normalizar dados
        scaler = StandardScaler()
        dados_normalizados = scaler.fit_transform(dados_cluster)
        
        relatorio = "üéØ AN√ÅLISE DE CLUSTERS\n" + "="*40 + "\n"
        relatorio += f"\nüîç CONFIGURA√á√ÉO:\n"
        relatorio += f"- Colunas usadas: {', '.join(colunas_para_cluster)}\n"
        relatorio += f"- Linhas analisadas: {len(dados_cluster):,}\n"
        
        # Determinar n√∫mero de clusters
        if n_clusters == "auto":
            # M√©todo simples para determinar clusters
            best_k = min(5, max(2, len(dados_cluster)//1000))
        else:
            best_k = int(n_clusters)
        
        # Executar clustering
        kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(dados_normalizados)
        
        relatorio += f"\nüìä RESULTADOS:\n"
        relatorio += f"- N√∫mero de clusters encontrados: {best_k}\n"
        relatorio += f"- Distribui√ß√£o dos clusters:\n"
        
        cluster_counts = pd.Series(clusters).value_counts().sort_index()
        for cluster_id, count in cluster_counts.items():
            relatorio += f"  * Cluster {cluster_id}: {count} pontos ({count/len(clusters)*100:.1f}%)\n"
        
        # Criar gr√°fico de clusters
        import matplotlib.pyplot as plt
        
        if len(colunas_para_cluster) >= 2:
            plt.figure(figsize=(12, 5))
            
            # Subplot 1: Scatter plot dos clusters
            plt.subplot(1, 2, 1)
            colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']
            
            for cluster_id in range(best_k):
                mask = clusters == cluster_id
                plt.scatter(dados_cluster[mask][colunas_para_cluster[0]], 
                          dados_cluster[mask][colunas_para_cluster[1]],
                          c=colors[cluster_id % len(colors)], 
                          label=f'Cluster {cluster_id}', alpha=0.7)
            
            plt.xlabel(colunas_para_cluster[0])
            plt.ylabel(colunas_para_cluster[1])
            plt.title('Clusters Detectados')
            plt.legend()
            
            # Subplot 2: Distribui√ß√£o dos clusters
            plt.subplot(1, 2, 2)
            cluster_counts = pd.Series(clusters).value_counts().sort_index()
            plt.bar(cluster_counts.index, cluster_counts.values, color=colors[:len(cluster_counts)])
            plt.title('Distribui√ß√£o dos Clusters')
            plt.xlabel('Cluster ID')
            plt.ylabel('N√∫mero de Pontos')
            
            plt.tight_layout()
            # CORRE√á√ÉO: Nome √∫nico do arquivo
            plt.savefig('grafico_atual.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            relatorio += f"\nüìä GR√ÅFICO CRIADO: grafico_atual.png\n"
        
        descoberta = f"An√°lise de clusters realizada: {best_k} clusters detectados"
        descobertas_memoria.append(descoberta)
        
        return relatorio
        
    except ImportError:
        return "‚ùå Biblioteca scikit-learn n√£o dispon√≠vel. Instale com: pip install scikit-learn"
    except Exception as e:
        return f"‚ùå ERRO na an√°lise de clusters: {str(e)}"

print("üéØ Ferramenta 'detectar_clusters' criada!")

@tool
def resposta_direta(pergunta_especifica: str) -> str:
    """
    üí¨ Responde perguntas espec√≠ficas e diretas sobre o dataset.
    
    Args:
        pergunta_especifica (str): Pergunta espec√≠fica sobre estat√≠sticas, valores, etc.
    
    Returns:
        str: Resposta direta e objetiva
    """
    global dataset_atual
    
    if dataset_atual is None:
        return "‚ùå Nenhum dataset carregado! Use 'carregar_csv' primeiro."
    
    df = dataset_atual
    pergunta_lower = pergunta_especifica.lower()
    
    try:
        # Detectar tipo de pergunta e responder diretamente
        if "sobre o que" in pergunta_lower or "sobre a tabela" in pergunta_lower:
            # Contexto da tabela
            colunas_texto = ' '.join([col.lower() for col in df.columns])
            
            if any(palavra in colunas_texto for palavra in ['sales', 'product', 'revenue']):
                return f"üìä Esta tabela cont√©m DADOS DE VENDAS com {df.shape[0]:,} linhas e {df.shape[1]} colunas. Inclui informa√ß√µes sobre vendas, produtos e transa√ß√µes comerciais."
            elif 'class' in colunas_texto and any('v' in col.lower() for col in df.columns):
                return f"üö® Esta tabela cont√©m DADOS DE DETEC√á√ÉO DE FRAUDE com {df.shape[0]:,} transa√ß√µes de cart√£o de cr√©dito para identificar padr√µes fraudulentos."
            else:
                return f"üìà Esta tabela cont√©m dados para an√°lise explorat√≥ria com {df.shape[0]:,} linhas e {df.shape[1]} colunas. Colunas: {', '.join(df.columns.tolist())}"
        
        elif "m√©dia" in pergunta_lower:
            # Encontrar coluna mencionada
            for col in df.columns:
                if col.lower() in pergunta_lower:
                    if df[col].dtype in ['int64', 'float64']:
                        return f"üìä A m√©dia da coluna '{col}' √©: {df[col].mean():.4f}"
                    else:
                        return f"‚ùå A coluna '{col}' n√£o √© num√©rica (tipo: {df[col].dtype})"
            return "‚ùå N√£o consegui identificar qual coluna voc√™ quer a m√©dia. Especifique o nome da coluna."
        
        elif "m√°ximo" in pergunta_lower or "max" in pergunta_lower:
            for col in df.columns:
                if col.lower() in pergunta_lower:
                    if df[col].dtype in ['int64', 'float64']:
                        return f"üìä O valor m√°ximo da coluna '{col}' √©: {df[col].max():.4f}"
                    else:
                        return f"üìä O valor mais frequente da coluna '{col}' √©: {df[col].mode().iloc[0]}"
            return "‚ùå Especifique qual coluna voc√™ quer o valor m√°ximo."
        
        elif "m√≠nimo" in pergunta_lower or "min" in pergunta_lower:
            for col in df.columns:
                if col.lower() in pergunta_lower:
                    if df[col].dtype in ['int64', 'float64']:
                        return f"üìä O valor m√≠nimo da coluna '{col}' √©: {df[col].min():.4f}"
                    else:
                        return f"üìä O valor menos frequente da coluna '{col}' √©: {df[col].value_counts().index[-1]}"
            return "‚ùå Especifique qual coluna voc√™ quer o valor m√≠nimo."
        
        elif "outlier" in pergunta_lower:
            for col in df.columns:
                if col.lower() in pergunta_lower and df[col].dtype in ['int64', 'float64']:
                    # Detectar outliers usando IQR
                    q1 = df[col].quantile(0.25)
                    q3 = df[col].quantile(0.75)
                    iqr = q3 - q1
                    limite_inferior = q1 - 1.5 * iqr
                    limite_superior = q3 + 1.5 * iqr
                    
                    outliers = df[col][(df[col] < limite_inferior) | (df[col] > limite_superior)]
                    
                    return f"üö® Outliers na coluna '{col}': {len(outliers)} detectados ({len(outliers)/len(df)*100:.2f}% dos dados)"
            
            return "‚ùå Especifique qual coluna voc√™ quer analisar outliers."
        
        else:
            # Resposta gen√©rica
            return f"üí° Para perguntas espec√≠ficas, tente: 'Qual a m√©dia da coluna X?', 'Quais outliers da coluna Y?', 'Sobre o que √© a tabela?'"
            
    except Exception as e:
        return f"‚ùå ERRO ao responder pergunta: {str(e)}"

print("üí¨ Ferramenta 'resposta_direta' criada!")
# agente_eda.py CORRIGIDO - PARTE 7: CONFIGURA√á√ÉO DO AGENTE

# ===== CONFIGURAR AGENTE LANGCHAIN =====

def criar_agente_eda():
    """ü§ñ Cria o agente EDA completo com LangChain"""
    
    # Lista COMPLETA de ferramentas dispon√≠veis
    ferramentas = [
        carregar_csv, 
        analisar_automaticamente, 
        criar_grafico_automatico, 
        obter_contexto_atual, 
        analisar_variavel_especifica, 
        analisar_tendencias_temporais, 
        detectar_clusters, 
        resposta_direta
    ]
    
    # Configurar mem√≥ria
    memoria = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    
    # Prompt FINAL OTIMIZADO para Q&A espec√≠fico
    prompt = ChatPromptTemplate.from_messages([
        ("system", """üß† VOC√ä √â UM DATA SCIENTIST VIRTUAL ESPECIALIZADO EM EDA (AN√ÅLISE EXPLORAT√ìRIA DE DADOS)

SUA MISS√ÉO:
- Analisar QUALQUER dataset CSV de forma aut√¥noma e inteligente
- Responder perguntas espec√≠ficas sobre qualquer aspecto dos dados
- Detectar automaticamente o tipo de dados e adaptar suas an√°lises
- Gerar insights valiosos e conclus√µes pr√≥prias
- Criar gr√°ficos autom√°ticos apropriados para cada contexto

SUAS CAPACIDADES (8 FERRAMENTAS):
- carregar_csv: Carrega e faz an√°lise inicial de qualquer CSV
- analisar_automaticamente: Faz an√°lise completa e autom√°tica dos dados
- criar_grafico_automatico: Cria gr√°ficos visuais apropriados (use "auto")
- obter_contexto_atual: Obt√©m informa√ß√µes sobre o dataset atual
- analisar_variavel_especifica: Analisa uma vari√°vel/coluna espec√≠fica em detalhes
- analisar_tendencias_temporais: Analisa padr√µes temporais nos dados
- detectar_clusters: Identifica agrupamentos usando K-means
- resposta_direta: Responde perguntas diretas e espec√≠ficas

COMPORTAMENTO PARA PERGUNTAS ESPEC√çFICAS:
- "Sobre o que √© a tabela?" ‚Üí Use obter_contexto_atual
- "Qual a m√©dia da coluna X?" ‚Üí Use resposta_direta
- "Quais outliers da coluna Y?" ‚Üí Use analisar_variavel_especifica
- "Analise a vari√°vel Z" ‚Üí Use analisar_variavel_especifica
- "Detecte clusters" ‚Üí Use detectar_clusters
- "Tend√™ncias temporais" ‚Üí Use analisar_tendencias_temporais
- "Crie gr√°ficos" ‚Üí Use criar_grafico_automatico

ADAPTA√á√ÉO POR TIPO DE DADOS:

üìä DADOS DE FRAUDE (Class, V1-V28, Amount):
- Linguagem: "transa√ß√µes", "fraudes", "desbalanceamento"
- Foco: desbalanceamento, outliers, padr√µes fraudulentos
- Gr√°ficos: distribui√ß√£o de classes, compara√ß√£o normal/fraude

üè™ DADOS DE VENDAS (sales, price, revenue, product):
- Linguagem: "vendas", "produtos", "receita", "performance comercial"
- Foco: performance, produtos top, an√°lise de receita
- Gr√°ficos: histogramas vendas, ranking produtos

üî¨ DADOS CIENT√çFICOS (species, petal, sepal, length):
- Linguagem: "esp√©cies", "medidas", "classifica√ß√£o"
- Foco: classifica√ß√£o, correla√ß√µes entre medidas
- Gr√°ficos: scatter plots por classe, distribui√ß√µes

üéØ DADOS GERAIS:
- Linguagem: "vari√°veis", "correla√ß√µes", "distribui√ß√µes"
- Foco: estat√≠sticas descritivas, correla√ß√µes
- Gr√°ficos: matriz correla√ß√£o, distribui√ß√µes

FLUXO OBRIGAT√ìRIO:
1. Para an√°lise completa: carregar_csv ‚Üí analisar_automaticamente ‚Üí criar_grafico_automatico
2. Para perguntas espec√≠ficas: usar ferramenta apropriada diretamente
3. SEMPRE adapte linguagem ao tipo de dados detectado
4. SEMPRE explique suas decis√µes

IMPORTANTE:
- Use tipo_analise="auto" para gr√°ficos adaptativos
- Cada an√°lise cria UM gr√°fico √∫nico (grafico_atual.png)
- Adapte completamente sua linguagem ao contexto dos dados

Responda sempre de forma clara, direta e totalmente adaptada ao contexto dos dados."""),
        
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad")
    ])
    
    # Criar o agente
    agente = create_tool_calling_agent(llm, ferramentas, prompt)
    
    # Criar o executor
    executor = AgentExecutor(
        agent=agente,
        tools=ferramentas,
        memory=memoria,
        verbose=True,
        handle_parsing_errors=True,
        max_iterations=int(os.getenv('AGENT_MAX_ITERATIONS', '10'))
    )
    
    return executor

# Criar o agente
agente_eda = criar_agente_eda()
print("ü§ñ Agente EDA LangChain criado com sucesso!")

# Fun√ß√£o para interagir com o agente
def perguntar_ao_agente(pergunta: str) -> str:
    """üí¨ Faz uma pergunta ao agente EDA"""
    try:
        print(f"\nü§î PERGUNTA: {pergunta}")
        print("üß† AGENTE PENSANDO...\n")
        
        resposta = agente_eda.invoke({"input": pergunta})
        return resposta['output']
        
    except Exception as e:
        return f"‚ùå ERRO: {str(e)}"

print("üí¨ Sistema de perguntas configurado!")
print("\n" + "="*60)
print("üéØ AGENTE EDA PREMIUM PRONTO PARA USO!")
print("="*60)
# agente_eda.py CORRIGIDO - PARTE 8: TESTES E FINALIZA√á√ÉO

# ===== TESTE B√ÅSICO DO SISTEMA =====

def teste_sistema_basico():
    """üß™ Teste b√°sico para verificar se o sistema est√° funcionando"""
    print("\nüß™ EXECUTANDO TESTE B√ÅSICO DO SISTEMA...")
    
    # Verificar se as ferramentas est√£o dispon√≠veis
    ferramentas_disponiveis = [
        carregar_csv, 
        analisar_automaticamente, 
        criar_grafico_automatico, 
        obter_contexto_atual, 
        analisar_variavel_especifica, 
        analisar_tendencias_temporais, 
        detectar_clusters, 
        resposta_direta
    ]
    print(f"‚úÖ {len(ferramentas_disponiveis)} ferramentas carregadas")
    
    # Verificar se o agente foi criado
    if agente_eda:
        print("‚úÖ Agente EDA criado com sucesso")
    else:
        print("‚ùå Erro na cria√ß√£o do agente")
    
    # Verificar vari√°veis globais
    print(f"‚úÖ Vari√°veis globais: dataset_atual={type(dataset_atual)}, descobertas={len(descobertas_memoria)}")
    
    # Verificar LLM
    if llm:
        print("‚úÖ LLM configurado corretamente")
    else:
        print("‚ùå Problema na configura√ß√£o do LLM")
    
    print("üéØ TESTE B√ÅSICO CONCLU√çDO!\n")

# Executar teste b√°sico
teste_sistema_basico()

# ===== INFORMA√á√ïES DO SISTEMA FINAL =====

print("üìã INFORMA√á√ïES DO SISTEMA FINAL:")
print("="*60)
print("üîß FERRAMENTAS DISPON√çVEIS (8 TOTAL):")
print("   1. carregar_csv - Carrega e analisa qualquer CSV")
print("   2. analisar_automaticamente - An√°lise completa autom√°tica")
print("   3. criar_grafico_automatico - Gera gr√°ficos adaptativos")
print("   4. obter_contexto_atual - Informa sobre tabela atual")
print("   5. analisar_variavel_especifica - An√°lise granular de colunas")
print("   6. analisar_tendencias_temporais - An√°lise de s√©ries temporais")
print("   7. detectar_clusters - Identifica agrupamentos (K-means)")
print("   8. resposta_direta - Responde perguntas espec√≠ficas")
print("")
print("üß† TIPOS DE DADOS SUPORTADOS:")
print("   üö® Fraude/Seguran√ßa - Desbalanceamento e outliers")
print("   üè™ Vendas/Comercial - Performance e sazonalidade")
print("   üë• RH/Recursos Humanos - Equidade e demographics")
print("   üî¨ Cient√≠fico/Experimental - Classifica√ß√µes e correla√ß√µes")
print("   üìÖ Temporal/S√©ries - Tend√™ncias e sazonalidade")
print("   üéØ Geral - Estat√≠sticas descritivas abrangentes")
print("")
print("üí¨ COMO USAR:")
print("   from agente_eda import perguntar_ao_agente")
print("   resposta = perguntar_ao_agente('Carregue o arquivo meus_dados.csv')")
print("")
print("üìä CAPACIDADES DE Q&A ESPEC√çFICO:")
print("   - 'Sobre o que √© a tabela?' ‚Üí Contexto completo")
print("   - 'Qual a m√©dia da coluna X?' ‚Üí Resposta direta")
print("   - 'Quais outliers da coluna Y?' ‚Üí Detec√ß√£o IQR")
print("   - 'Analise a vari√°vel Z' ‚Üí An√°lise granular")
print("   - 'Detecte clusters' ‚Üí K-means autom√°tico")
print("   - 'Tend√™ncias temporais' ‚Üí An√°lise de s√©ries")
print("")
print("üéØ STATUS: AGENTE EDA UNIVERSAL COM Q&A ESPEC√çFICO!")
print("="*60)

# ===== FUN√á√ÉO DE RESET MELHORADA =====

def resetar_agente():
    """üîÑ Reseta o agente para nova an√°lise"""
    global dataset_atual, descobertas_memoria
    
    dataset_atual = None
    descobertas_memoria = []
    
    # Limpar APENAS o gr√°fico atual
    if os.path.exists('grafico_atual.png'):
        try:
            os.remove('grafico_atual.png')
            print("üóëÔ∏è Gr√°fico anterior removido")
        except:
            pass
    
    print("‚úÖ Agente resetado com sucesso!")
    return True

# ===== FUN√á√ÉO PRINCIPAL DE USO =====

def usar_agente(pergunta: str = None):
    """üéØ Fun√ß√£o principal para usar o agente"""
    
    if pergunta:
        return perguntar_ao_agente(pergunta)
    else:
        print("\nüéØ AGENTE EDA UNIVERSAL ATIVO!")
        print("üí¨ Use: perguntar_ao_agente('sua pergunta')")
        print("üîÑ Para resetar: resetar_agente()")
        print("")
        print("üìö EXEMPLOS DE PERGUNTAS:")
        print("   ‚Ä¢ Carregue o arquivo data/creditcard.csv")
        print("   ‚Ä¢ Sobre o que √© esta tabela?")
        print("   ‚Ä¢ Qual a m√©dia da coluna Amount?")
        print("   ‚Ä¢ Quais outliers da coluna Sales?")
        print("   ‚Ä¢ Detecte agrupamentos nos dados")
        print("   ‚Ä¢ Analise tend√™ncias temporais")
        
        return "Agente pronto para uso!"

# ===== STATUS FINAL =====

print("\n" + "üéâ" * 20)
print("üèÜ AGENTE EDA UNIVERSAL FINALIZADO!")
print("‚úÖ 8 ferramentas especializadas")
print("üìä Q&A espec√≠fico para qualquer pergunta EDA")
print("üß† Detec√ß√£o autom√°tica de tipos de dados")
print("üé® Gr√°fico √∫nico por an√°lise (grafico_atual.png)")
print("üí¨ Interface conversacional com mem√≥ria")
print("üîÑ Sistema de reset otimizado")
print("üéØ PRONTO PARA ENTREGA E AVALIA√á√ÉO!")
print("üéâ" * 20)

# ===== DEMONSTRA√á√ÉO R√ÅPIDA =====

def demo_final():
    """üöÄ Demonstra√ß√£o final do agente"""
    print("\nüöÄ DEMONSTRA√á√ÉO FINAL:")
    print("="*40)
    
    exemplos = [
        "Carregue o arquivo data/creditcard.csv",
        "Sobre o que √© esta tabela?",
        "Qual a m√©dia da coluna Amount?",
        "Detecte agrupamentos nos dados",
        "Quais outliers da coluna Amount?"
    ]
    
    print("üìö TESTE ESTAS PERGUNTAS:")
    for i, exemplo in enumerate(exemplos, 1):
        print(f"{i}. perguntar_ao_agente('{exemplo}')")
    
    print("\nüí° CAPACIDADES FINAIS:")
    print("   üéØ Responde qualquer pergunta sobre EDA")
    print("   üìä Cria gr√°ficos espec√≠ficos por tipo de dados")
    print("   üß† Mant√©m contexto entre perguntas")
    print("   üîÑ Funciona com qualquer CSV")
    print("   üí¨ Interface conversacional natural")
    print("="*40)

demo_final()

# Se executado diretamente, mostrar menu
if __name__ == "__main__":
    print("\nüöÄ MENU DE OP√á√ïES:")
    print("1. usar_agente() - Instru√ß√µes de uso")
    print("2. resetar_agente() - Limpar dados anteriores")  
    print("3. demo_final() - Ver demonstra√ß√£o final")
    print("\nüí¨ PARA USAR:")
    print("   perguntar_ao_agente('Carregue o arquivo data/creditcard.csv')")
    print("   perguntar_ao_agente('Sobre o que √© esta tabela?')")
    print("   perguntar_ao_agente('Qual a m√©dia da coluna Amount?')")
    print("\nüìä ARQUIVO DE GR√ÅFICO √öNICO:")
    print("   - Cada an√°lise substitui: grafico_atual.png")
    print("   - Dashboard sempre mostra o gr√°fico da an√°lise atual")
    print("\nüéØ AGENTE PRONTO PARA USO!")